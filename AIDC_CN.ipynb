{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1xWRo1gLxtM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from collections import OrderedDict\n",
        "from torch import Tensor\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import glob\n",
        "import sklearn\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Subset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import random\n",
        "from google.colab import files\n",
        "from sklearn.metrics import  confusion_matrix\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.ndimage import distance_transform_edt\n",
        "from torch.autograd import Variable\n",
        "import skimage.segmentation\n",
        "import skimage.io\n",
        "import skimage\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "from skimage import feature\n",
        "from skimage import filters\n",
        "import copy\n",
        "import torchvision\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "import torch\n",
        "import scipy\n",
        "from scipy import signal as signal\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import FastICA, PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from scipy.signal import ellip, lfilter\n",
        "from scipy.signal import freqz\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TFpnT0odJbS"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "import torch_geometric.nn as gnn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import DataLoader as DataLoader_graph\n",
        "from torch_geometric.nn import global_mean_pool,global_max_pool,ChebConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LogzPy2iXrjD"
      },
      "outputs": [],
      "source": [
        "#dataset_parameters\n",
        "\n",
        "dataset_parameter = dict(\n",
        "    fs = 250,\n",
        "    channel = 22,\n",
        "    num_classes = 4,\n",
        "    num_subject = 10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGJ6H89fUFL8"
      },
      "outputs": [],
      "source": [
        "#model_parameters\n",
        "\n",
        "parameter = dict(\n",
        "N = 5000,\n",
        "F = 1024,\n",
        "P = 784,\n",
        "H = 28,\n",
        "W = 28,\n",
        "kH = 9,\n",
        "kW =9,\n",
        "kH1 = 9,\n",
        "kW1 = 9,\n",
        "C11 = 32,\n",
        "C12 = 8,\n",
        "C3 =16,\n",
        "C_FC1 = 512,\n",
        "C_FC2 = 1024,\n",
        "m_plus = 0.9,\n",
        "m_minus = 0.1,\n",
        "alpha = 0.0001,\n",
        "lamda = 0.5,\n",
        "batch = 4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_loading\n",
        "\n",
        "dframe = pd.DataFrame\n",
        "ns = dataset_parameter['num_subject']\n",
        "ori_data1 = dict()\n",
        "mod_data1 = dict()\n",
        "def subjectCounter(i):\n",
        "    return 'subject0{}'.format(i)\n",
        "\n",
        "for i in range(1, ns):\n",
        "    data_path1 = '/content/A0'+str(i)+'T.npz'.format(i)\n",
        "    subj = subjectCounter(i)\n",
        "    ori_data1[subj] = np.load(data_path1)\n",
        "    mod_data1[subj] = {}\n",
        "    mod_data1[subj]['raw_EEG'] = ori_data1[subj]['s']\n",
        "pd.DataFrame(mod_data1['subject01']['raw_EEG']).head()\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    mod_data1[subj]['raw_EEG'] = np.delete(mod_data1[subj]['raw_EEG'], np.s_[22:], 1)\n",
        "    print(mod_data1['subject01']['raw_EEG'].shape)\n",
        "\n",
        "#data_filtering\n",
        "\n",
        "lowcut=4\n",
        "highcut=40\n",
        "fs = dataset_parameter['fs']\n",
        "\n",
        "def elliptical_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    ripple = 0.001\n",
        "    attenuation = 100\n",
        "    low = lowcut/nyq\n",
        "    high = highcut/nyq\n",
        "    b,a = ellip(order, ripple, attenuation, [low, high], btype='band')\n",
        "    y = lfilter(b, a, signal)\n",
        "    return y\n",
        "\n",
        "def elliptical_bandpass_one_subject(data, subj, lowcut, highcut, fs, interval=None):\n",
        "    print('Processing ', subj)\n",
        "    data[subj]['EEG_filtered'] = {}\n",
        "    temp_raw_EEG = data[subj]['raw_EEG']\n",
        "    if interval is not None:\n",
        "        startband = np.arange(lowcut, highcut, step = interval)\n",
        "        for start in startband:\n",
        "            band = \"{:02d}_{:02d}\".format(start, start+interval)\n",
        "            print('Filtering through {} Hz band'.format(band))\n",
        "            data[subj]['EEG_filtered'][band] = {}\n",
        "            data[subj]['EEG_filtered'][band]['EEG_all'] = elliptical_bandpass_filter(temp_raw_EEG, start, start+interval, fs)\n",
        "    else:\n",
        "        band = \"{:02d}_{:02d}\".format(lowcut, highcut)\n",
        "        data[subj]['EEG_filtered'][band]['EEG_all'] = elliptical_bandpass_filter(temp_raw_EEG, lowcut, highcut, fs)\n",
        "\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    elliptical_bandpass_one_subject(mod_data1, subj, lowcut, highcut, fs, interval=36)\n",
        "    print('')\n",
        "\n",
        "#seggregating data class-wise\n",
        "\n",
        "left_pos = 'left_pos'\n",
        "right_pos = 'right_pos'\n",
        "tongue_pos = 'tongue_pos'\n",
        "feet_pos = 'feet_pos'\n",
        "start = 0.5\n",
        "end = 3.5\n",
        "EEG_left = 'EEG_left'\n",
        "EEG_right = 'EEG_right'\n",
        "EEG_tongue = 'EEG_tongue'\n",
        "EEG_feet = 'EEG_feet'\n",
        "for i in range(1, ns):\n",
        "  subj = subjectCounter(i)\n",
        "  mod_data1[subj][left_pos] = ori_data1[subj]['epos'][ori_data1[subj]['etyp'] == 769]\n",
        "  mod_data1[subj][right_pos] = ori_data1[subj]['epos'][ori_data1[subj]['etyp'] == 770]\n",
        "  mod_data1[subj][tongue_pos] = ori_data1[subj]['epos'][ori_data1[subj]['etyp'] == 772]\n",
        "  mod_data1[subj][feet_pos] = ori_data1[subj]['epos'][ori_data1[subj]['etyp'] == 771]\n",
        "for i in range(1, ns):\n",
        "  subj = subjectCounter(i)\n",
        "  print('Processing ', subj)\n",
        "  temp_pos_left1 = mod_data1[subj][left_pos]\n",
        "  temp_pos_right1 = mod_data1[subj][right_pos]\n",
        "  temp_pos_tongue1 = mod_data1[subj][tongue_pos]\n",
        "  temp_pos_feet1 = mod_data1[subj][feet_pos]\n",
        "  for band in mod_data1[subj]['EEG_filtered'].keys():\n",
        "    temp_EEG_all = mod_data1[subj]['EEG_filtered'][band]['EEG_all']\n",
        "    temp_EEG_left1 = []\n",
        "    temp_EEG_right1 = []\n",
        "    temp_EEG_tongue1 = []\n",
        "    temp_EEG_feet1 = []\n",
        "    for j in range(len(temp_pos_left1)):\n",
        "        temp_EEG_left1.append(temp_EEG_all[temp_pos_left1[j] + int(start*fs) : temp_pos_left1[j] + int(end*fs),:])\n",
        "    mod_data1[subj]['EEG_filtered'][band]['EEG_left'] = np.array(temp_EEG_left1)\n",
        "    for j in range(len(temp_pos_right1)):\n",
        "        temp_EEG_right1.append(temp_EEG_all[temp_pos_right1[j] + int(start*fs) : temp_pos_right1[j] + int(end*fs),:])\n",
        "    mod_data1[subj]['EEG_filtered'][band]['EEG_right'] = np.array(temp_EEG_right1)\n",
        "    for j in range(len(temp_pos_tongue1)):\n",
        "        temp_EEG_tongue1.append(temp_EEG_all[temp_pos_tongue1[j] + int(start*fs) : temp_pos_tongue1[j] + int(end*fs),:])\n",
        "    mod_data1[subj]['EEG_filtered'][band]['EEG_tongue'] = np.array(temp_EEG_tongue1)\n",
        "    for j in range(len(temp_pos_feet1)):\n",
        "        temp_EEG_feet1.append(temp_EEG_all[temp_pos_feet1[j] + int(start*fs) : temp_pos_feet1[j] + int(end*fs),:])\n",
        "    mod_data1[subj]['EEG_filtered'][band]['EEG_feet'] = np.array(temp_EEG_feet1)\n",
        "  mod_data1[subj]['EEG_filtered'][band]\n",
        "\n",
        "#train_test_data_split\n",
        "\n",
        "def split_EEG_one_class(EEG_one_class,ratio=0.5):\n",
        "    nr = int(ratio * np.shape(EEG_one_class)[0])\n",
        "    EEG_train = EEG_one_class[:nr,:,:]\n",
        "    EEG_test = EEG_one_class[nr:,:,:]\n",
        "    return EEG_train, EEG_test\n",
        "\n",
        "for i in range(1, ns):\n",
        "    subj = subjectCounter(i)\n",
        "    temp_EEG_left1 = mod_data1[subj]['EEG_filtered']['04_40']['EEG_left']\n",
        "    temp_EEG_right1 = mod_data1[subj]['EEG_filtered']['04_40']['EEG_right']\n",
        "    temp_EEG_tongue1 = mod_data1[subj]['EEG_filtered']['04_40']['EEG_tongue']\n",
        "    temp_EEG_feet1 = mod_data1[subj]['EEG_filtered']['04_40']['EEG_feet']\n",
        "    temp_filt = mod_data1\n",
        "    temp_filt[subj]['EEG_left_train'], temp_filt[subj]['EEG_left_test'] = split_EEG_one_class(temp_EEG_left1,ratio = 0.5)\n",
        "    temp_filt[subj]['EEG_right_train'], temp_filt[subj]['EEG_right_test'] = split_EEG_one_class(temp_EEG_right1,ratio = 0.5)\n",
        "    temp_filt[subj]['EEG_tongue_train'], temp_filt[subj]['EEG_tongue_test'] = split_EEG_one_class(temp_EEG_tongue1,ratio = 0.5)\n",
        "    temp_filt[subj]['EEG_feet_train'], temp_filt[subj]['EEG_feet_test'] = split_EEG_one_class(temp_EEG_feet1,ratio = 0.5)\n",
        "\n",
        "#Subject specific data\n",
        "\n",
        "subj = 'subject01'\n",
        "t_train = np.concatenate((temp_filt[subj]['EEG_left_train'],temp_filt[subj]['EEG_right_train'],temp_filt[subj]['EEG_tongue_train'],temp_filt[subj]['EEG_feet_train']), axis=0)\n",
        "t_test = np.concatenate((temp_filt[subj]['EEG_left_test'],temp_filt[subj]['EEG_right_test'],temp_filt[subj]['EEG_tongue_test'],temp_filt[subj]['EEG_feet_test']), axis=0)\n",
        "t_train = np.reshape(t_train, [np.shape(t_train)[0],np.shape(t_train)[2],np.shape(t_train)[1]])\n",
        "t_test = np.reshape(t_test, [np.shape(t_test)[0],np.shape(t_test)[2],np.shape(t_test)[1]])\n",
        "\n",
        "p1 = [0]*np.shape(temp_filt[subj]['EEG_left_train'])[0]\n",
        "p2 = [1]*np.shape(temp_filt[subj]['EEG_right_train'])[0]\n",
        "p3 = [2]*np.shape(temp_filt[subj]['EEG_tongue_train'])[0]\n",
        "p4 = [3]*np.shape(temp_filt[subj]['EEG_feet_train'])[0]\n",
        "label_train = np.concatenate((p1,p2,p3,p4), axis=0)\n",
        "\n",
        "p1 = [0]*np.shape(temp_filt[subj]['EEG_left_test'])[0]\n",
        "p2 = [1]*np.shape(temp_filt[subj]['EEG_right_test'])[0]\n",
        "p3 = [2]*np.shape(temp_filt[subj]['EEG_tongue_test'])[0]\n",
        "p4 = [3]*np.shape(temp_filt[subj]['EEG_feet_test'])[0]\n",
        "label_test = np.concatenate((p1,p2,p3,p4), axis=0)\n",
        "\n",
        "print(np.shape(t_train))\n",
        "print(np.shape(t_test))\n",
        "print(np.shape(label_train))\n",
        "print(np.shape(label_test))\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_paths, target_paths):\n",
        "\n",
        "        self.input_paths = input_paths\n",
        "        self.target_paths = target_paths\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        p = np.shape(self.input_paths[index])[0]\n",
        "        input = self.input_paths[index]\n",
        "        input = torch.from_numpy(input)\n",
        "        classes = torch.from_numpy(np.array(self.target_paths[index]))\n",
        "        return input,classes\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_paths)\n",
        "\n",
        "train_dataset = CustomDataset(t_train, label_train)\n",
        "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=parameter['batch'], shuffle=True, num_workers=2)\n",
        "test_dataset = CustomDataset(t_test, label_test)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=parameter['batch'], shuffle=False, num_workers=2)\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(np.shape(train_dataset[0][0]))\n",
        "print(np.shape(test_dataset[0][0]))"
      ],
      "metadata": {
        "id": "hXU4Xm8uUOo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_enable = 1\n",
        "device = torch.device(\"cuda\" if cuda_enable else \"cpu\")"
      ],
      "metadata": {
        "id": "GdH4DjwmPXX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91BMIVgkkj5j"
      },
      "outputs": [],
      "source": [
        "#defining edges for graph\n",
        "\n",
        "def edge_idx(x):\n",
        "  edges=[]\n",
        "  for i in range(dataset_parameter['channel']):\n",
        "    for j in range(dataset_parameter['channel']):\n",
        "      y=[i]\n",
        "      if i!=j:\n",
        "        y.append(j)\n",
        "        edges.append(y)\n",
        "  edges=torch.tensor(edges)\n",
        "  return edges\n",
        "def edge_wt(x,edge):\n",
        "  weights=torch.zeros(((dataset_parameter['channel'])*(dataset_parameter['channel']-1),2))\n",
        "  k = torch.zeros((dataset_parameter['channel'])*(dataset_parameter['channel']-1))\n",
        "  x_mean=torch.mean(x,0,keepdim=True)\n",
        "  x_std=torch.std(x,0,keepdim=True)\n",
        "  x=(x-x_mean)/x_std\n",
        "  for i in range(edge.shape[0]):\n",
        "    print()\n",
        "    output=torch.mean((x[edge[i][0],:]*x[edge[i][1],:]))\n",
        "    weights[i,0]=output\n",
        "    k[i] = output\n",
        "  return k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw3ZVt8RaIWz"
      },
      "outputs": [],
      "source": [
        "###graph_model\n",
        "\n",
        "class graph_model(nn.Module):\n",
        "  def __init__(self,N,F,batch,height,width):\n",
        "    super(graph_model,self).__init__()\n",
        "    self.batch = batch\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    P = int(height*width)\n",
        "    self.conv1=gnn.ChebConv(N, F, bool=False, K =2, normalization = None)\n",
        "    self.conv2=gnn.ChebConv(F, P, bool=False, K = 2, normalization = None)\n",
        "    self.fc=nn.Linear(P, 1)\n",
        "\n",
        "  def forward(self, x, edge_index, edge_weight):\n",
        "    B = torch.tensor([0]*dataset_parameter['channel'])\n",
        "    x = x.cpu()\n",
        "    edge_index = edge_index.cpu()\n",
        "    edge_weight = edge_weight.cpu()\n",
        "    x = self.conv1(x, edge_index, edge_weight, B)\n",
        "    x = self.conv2(x, edge_index, edge_weight, B)\n",
        "    x_out_mean=torch.reshape(gnn.global_mean_pool(x, B),[self.batch,1,self.height,self.width])\n",
        "    x_out_max=torch.reshape(gnn.global_max_pool(x,B),[self.batch,1,self.height,self.width])\n",
        "    preds = self.fc(torch.flatten((x_out_mean+x_out_max),start_dim = 1))\n",
        "    return x_out_mean, x_out_max, torch.sigmoid(preds)\n",
        "\n",
        "class graph_att(nn.Module):\n",
        "  def __init__(self,input_features,F,batch,height,width,device):\n",
        "    super(graph_att,self).__init__()\n",
        "    self.device = device\n",
        "    self.graph_block=graph_model(input_features,F,batch,height,width)\n",
        "    self.batch = batch\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "\n",
        "  def forward(self,x1,x2,x_graph,edge_index,edge_weight):\n",
        "    x_g_mean,x_g_max,preds=self.graph_block(x_graph,edge_index,edge_weight)\n",
        "    x_g_mean=torch.reshape(x_g_mean,(self.batch,1,self.height,self.width))\n",
        "    x_g_max=torch.reshape(x_g_max,(self.batch,1,self.height,self.width))\n",
        "    x=torch.cat((x1*(x_g_mean.to(self.device)),x2*(x_g_max.to(self.device))),1)\n",
        "    return x, preds.to(self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAeQzUddgZge"
      },
      "outputs": [],
      "source": [
        "###graph_2d_conv_attention\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        avg_out = self.avg_pool(x)\n",
        "        max_out = self.max_pool(x)\n",
        "        out = avg_out + max_out\n",
        "        return F.relu(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x = avg_out + max_out\n",
        "        return F.relu(x)\n",
        "\n",
        "class attlayer(nn.Module):\n",
        "    def __init__(self, in_channel):\n",
        "        super(attlayer, self).__init__()\n",
        "        self.ca=ChannelAttention(in_planes=in_channel)\n",
        "        self.sa=SpatialAttention()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat((self.ca(x)*x,self.sa(x)*x),1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MKNir2igb_U"
      },
      "outputs": [],
      "source": [
        "###decoder_model\n",
        "\n",
        "def mask(out_digit_caps,device):\n",
        "    v_length = torch.sqrt((out_digit_caps**2).sum(dim=2))\n",
        "    _, max_index = v_length.max(dim=1)\n",
        "    max_index = max_index.data\n",
        "    batch_size = out_digit_caps.size(0)\n",
        "    masked_v = [None] * batch_size\n",
        "    for batch_ix in range(batch_size):\n",
        "        sample = out_digit_caps[batch_ix]\n",
        "        v = Variable(torch.zeros(sample.size()))\n",
        "        v = v.to(device)\n",
        "        max_caps_index = max_index[batch_ix]\n",
        "        v[max_caps_index] = sample[max_caps_index]\n",
        "        masked_v[batch_ix] = v\n",
        "    masked = torch.stack(masked_v, dim=0)\n",
        "    return masked\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, num_classes, output_unit_size, input_height, input_width,num_channel,\n",
        "                 fc1_output_size, fc2_output_size,device):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.input_height = input_height\n",
        "        self.input_width = input_width\n",
        "        self.fc1 = nn.Linear(num_classes * output_unit_size, fc1_output_size).to(self.device)\n",
        "        self.fc2 = nn.Linear(fc1_output_size, fc2_output_size).to(self.device)\n",
        "        self.fc3 = nn.Linear(fc2_output_size, int(input_height*input_width)).to(self.device)\n",
        "        self.conv1 = nn.Conv2d(1,num_channel,kernel_size=3,stride=1,padding=1,bias=False,device=self.device)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        batch_size = target.size(0)\n",
        "        masked_caps = mask(x,self.device)\n",
        "        vector_j = masked_caps.view(x.size(0), -1)\n",
        "        fc1_out = self.relu(self.fc1(vector_j))\n",
        "        fc2_out = self.relu(self.fc2(fc1_out))\n",
        "        fc3_out = self.relu(self.fc3(fc2_out))\n",
        "        output = torch.reshape(fc3_out,(batch_size,1,self.input_height, self.input_width))\n",
        "        reconstruction = self.conv1(output)\n",
        "        return reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_0_mhCDQWOC"
      },
      "outputs": [],
      "source": [
        "###capsule_network\n",
        "\n",
        "def squash(sj, dim=2):\n",
        "    sj_mag_sq = torch.sum(sj**2, dim, keepdim=True)\n",
        "    sj_mag = torch.sqrt(sj_mag_sq)\n",
        "    v_j = (sj_mag_sq / (1.0 + sj_mag_sq)) * (sj / sj_mag)\n",
        "    return v_j\n",
        "\n",
        "class CapsuleLayer(nn.Module):\n",
        "    def __init__(self, in_unit, in_channel, num_unit, unit_size, primary_capsule_channel, kH, kW,  kH1, kW1, use_routing,\n",
        "                 num_routing,device):\n",
        "        super(CapsuleLayer, self).__init__()\n",
        "        self.in_unit = in_unit\n",
        "        self.in_channel = in_channel\n",
        "        self.num_unit = num_unit\n",
        "        self.primary_capsule_channel = primary_capsule_channel\n",
        "        self.use_routing = use_routing\n",
        "        self.num_routing = num_routing\n",
        "        self.device = device\n",
        "        if self.use_routing:\n",
        "            self.weight = nn.Parameter(torch.randn(1, in_channel, num_unit, unit_size, in_unit))\n",
        "        else:\n",
        "            self.conv0 = nn.Conv2d(self.in_channel,self.num_unit*self.primary_capsule_channel,kernel_size=[kH,kW],stride=1,padding=0,device=self.device)\n",
        "            self.conv_units = nn.ModuleList([\n",
        "                nn.Conv2d(self.num_unit*self.primary_capsule_channel, self.primary_capsule_channel, kernel_size=[kH1,kW1], stride=2,padding=0,device=self.device) for u in range(self.num_unit)\n",
        "            ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_routing:\n",
        "            return self.routing(x)\n",
        "        else:\n",
        "            return self.no_routing(x)\n",
        "\n",
        "    def routing(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = torch.stack([x] * self.num_unit, dim=2).unsqueeze(4)\n",
        "        batch_weight = torch.cat([self.weight] * batch_size, dim=0)\n",
        "        u_hat = torch.matmul(batch_weight.to(self.device), x)\n",
        "        b_ij = Variable(torch.zeros(1, self.in_channel, self.num_unit, 1))\n",
        "        b_ij = b_ij.to(self.device)\n",
        "        num_iterations = self.num_routing\n",
        "        c_ij=Variable(torch.ones((1, self.in_channel, self.num_unit, 1)))\n",
        "        c_ij = c_ij.to(self.device)\n",
        "        for iteration in range(num_iterations):\n",
        "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
        "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
        "            v_j = squash(s_j, dim=3)\n",
        "            v_j1 = torch.cat([v_j] * self.in_channel, dim=1)\n",
        "            u_vj1 = torch.matmul(u_hat.transpose(3, 4), v_j1).squeeze(4).mean(dim=0, keepdim=True)\n",
        "            b_ij = b_ij + u_vj1\n",
        "            c_ij = F.mish(b_ij,inplace=False)\n",
        "        return v_j.squeeze(1)\n",
        "\n",
        "    def no_routing(self, x):\n",
        "        x = self.conv0(x)\n",
        "        unit = [self.conv_units[i](x) for i, l in enumerate(self.conv_units)]\n",
        "        unit = torch.stack(unit, dim=1)\n",
        "        batch_size = x.size(0)\n",
        "        unit = unit.view(batch_size, self.num_unit, -1)\n",
        "        return squash(unit, dim=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBOWCaz3ZchS"
      },
      "outputs": [],
      "source": [
        "# AIDC-CN\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, graph_input_feature,\n",
        "                 num_conv_channel, num_primary_unit,\n",
        "                 primary_unit_size, primary_capsule_channel, kH, kW, kH1, kW1, num_classes, output_unit_size, num_routing,\n",
        "                 input_height,input_width,F,batch, fc1_output_size, fc2_output_size,device):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.loss_ce=nn.CrossEntropyLoss().to(self.device)\n",
        "        self.num_classes = num_classes\n",
        "        self.conv0 = nn.Conv2d(in_channels=num_conv_channel,\n",
        "                               out_channels=num_conv_channel,\n",
        "                               kernel_size=3,\n",
        "                               stride=1,padding=1,device=self.device)\n",
        "        self.self_att = attlayer(num_conv_channel)\n",
        "        self.graph_att =  graph_att(graph_input_feature,F,batch,input_height,input_width,self.device)\n",
        "        self.primary = CapsuleLayer(in_unit=0,\n",
        "                                    in_channel=4*num_conv_channel,\n",
        "                                    num_unit=num_primary_unit,\n",
        "                                    unit_size=primary_unit_size,\n",
        "                                    primary_capsule_channel = primary_capsule_channel,\n",
        "                                    kH = kH, kW = kW, kH1 = kH1, kW1 = kW1,\n",
        "                                    use_routing=False,\n",
        "                                    num_routing=num_routing,\n",
        "                                    device=self.device)\n",
        "        self.digits = CapsuleLayer(in_unit=num_primary_unit,\n",
        "                                   in_channel=primary_unit_size,\n",
        "                                   num_unit=num_classes,\n",
        "                                   unit_size=output_unit_size,\n",
        "                                   primary_capsule_channel = 0,\n",
        "                                   kH = 0, kW = 0, kH1 = 0, kW1 = 0,\n",
        "                                   use_routing=True,\n",
        "                                   num_routing=num_routing,\n",
        "                                   device=self.device)\n",
        "        self.decoder = Decoder(num_classes, output_unit_size,\n",
        "                                   input_height, input_width, num_conv_channel, fc1_output_size, fc2_output_size, self.device)\n",
        "\n",
        "    def forward(self, x1, edge_index, edge_weight, x2, label):\n",
        "        out_conv = F.relu(self.conv0(x2))\n",
        "        out_conv1 = self.self_att(out_conv)\n",
        "        out_conv11 = F.relu(self.conv0(x2))\n",
        "        out_conv12 = F.relu(self.conv0(x2))\n",
        "        out_conv2, preds_graph = self.graph_att(out_conv11,out_conv12, x1, edge_index, edge_weight)\n",
        "        input_caps=torch.cat((out_conv1,out_conv2), 1)\n",
        "        out_primary_caps = self.primary(input_caps)\n",
        "        out_digit_caps = self.digits(out_primary_caps)\n",
        "        label1 = torch.cat((label,torch.tensor([i for i in range(1, self.num_classes)]).to(self.device)))\n",
        "        label2=F.one_hot(label1.to(torch.int64),num_classes=self.num_classes)\n",
        "        target=label2[:self.num_classes]\n",
        "        recon=self.decoder(out_digit_caps,target)\n",
        "        return out_digit_caps,recon,preds_graph,target\n",
        "\n",
        "    def loss(self, out_digit_caps, graph_preds, recon, input_img, target, label,alpha,m_plus,m_minus,loss_lambda, size_average=True):\n",
        "        m_loss = self.margin_loss(out_digit_caps, target,m_plus,m_minus,loss_lambda)\n",
        "        rec_loss = self.recloss(recon, input_img)\n",
        "        if size_average:\n",
        "            m_loss = m_loss.mean()\n",
        "            rec_loss = rec_loss.mean()\n",
        "        graph_loss=self.loss_ce(graph_preds, label.unsqueeze(1))\n",
        "        return alpha*m_loss + rec_loss + graph_loss\n",
        "\n",
        "    def margin_loss(self, input, target,m_plus,m_minus,loss_lambda):\n",
        "        batch_size = input.size(0)\n",
        "        v_c = torch.sqrt((input**2).sum(dim=2, keepdim=True))\n",
        "        zero = Variable(torch.zeros(1))\n",
        "        zero = zero.to(device)\n",
        "        max_left = torch.max(m_plus - v_c, zero).view(batch_size, -1)**2\n",
        "        max_right = torch.max(v_c - m_minus, zero).view(batch_size, -1)**2\n",
        "        t_c = target\n",
        "        l_c = t_c * max_left + loss_lambda * (1.0 - t_c) * max_right\n",
        "        l_c = l_c.sum(dim=1)\n",
        "        return l_c\n",
        "\n",
        "    def recloss(self, recon, input_img):\n",
        "        image = input_img\n",
        "        batch_size=image.shape[0]\n",
        "        image=image.view(batch_size,-1)\n",
        "        recon=recon.view(batch_size,-1)\n",
        "        error=recon - image\n",
        "        sq_error=error**2\n",
        "        recon_error = torch.mean(sq_error, dim=1)\n",
        "        return recon_error\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(parameter['N'],dataset_parameter['channel'],parameter['C12'],1,parameter['C11'],parameter['kH'],parameter['kW'],parameter['kH1'],parameter['kW1'],dataset_parameter['num_classes'],parameter['C3'],3,parameter['H'],parameter['W'],parameter['F'],parameter['batch'],parameter['C_FC1'],parameter['C_FC2'],device)"
      ],
      "metadata": {
        "id": "p_Rs3EmJWxS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I49TA2MnaUh"
      },
      "outputs": [],
      "source": [
        "#evaluation_metrics\n",
        "\n",
        "class metrics(nn.Module):\n",
        "    def __init__(self, threshold=0.5):\n",
        "        super(metrics, self).__init__()\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, target, input):\n",
        "        eps = 1e-10\n",
        "        input_ = (input > self.threshold).data.float()\n",
        "        target_ = (target > self.threshold).data.float()\n",
        "        intersection = torch.clamp(input_ * target_, 0, 1)\n",
        "        union = torch.clamp(input_ + target_, 0, 1)\n",
        "        if torch.mean(intersection).lt(eps):\n",
        "            return torch.Tensor([0., 0., 0., 0.])\n",
        "        else:\n",
        "            acc = torch.mean((input_ == target_).data.float())\n",
        "            iou = torch.mean(intersection) / torch.mean(union)\n",
        "            recall = torch.mean(intersection) / torch.mean(target_)\n",
        "            precision = torch.mean(intersection) / torch.mean(input_)\n",
        "            f1_score = 2*recall*precision/(recall+precision)\n",
        "            return torch.Tensor([acc, f1_score])\n",
        "metrics=metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJF_1qC25VEP"
      },
      "outputs": [],
      "source": [
        "#training_testing_module\n",
        "\n",
        "def train_one_epoch_net(model, train_dl, learn, device):\n",
        "    opt = torch.optim.Adam(model.parameters(),lr=learn)\n",
        "    running_loss_image=0.0\n",
        "    acc_epoch=0.0\n",
        "    f1_epoch=0.0\n",
        "    dice_epoch=0.0\n",
        "    for a,b in train_dl:\n",
        "        a=a.float()\n",
        "        a=torch.nn.functional.interpolate(a, size=(parameter['N']), scale_factor=None, mode='nearest')\n",
        "        b=b.float()\n",
        "\n",
        "        ###preparing inputs for model (graph and STFT preparation)\n",
        "        edge_index = [(edge_idx(a[i])) for i in range(len(a))]\n",
        "        edges_idx = torch.reshape((torch.cat(edge_index,axis = 0)),[parameter['batch'],np.shape(edge_index[0])[0],np.shape(edge_index[0])[1]])\n",
        "        edge_weight = [(edge_wt(a[i],edges_idx[i])) for i in range(len(a))]\n",
        "        edges_wt = torch.reshape((torch.cat(edge_weight,axis = 0)),[parameter['batch'],np.shape(edge_weight[0])[0]])\n",
        "        p = [torch.abs(torch.stft(a[i],200,return_complex=True)) for i in range(len(a))]\n",
        "        q = torch.reshape((torch.cat(p,axis = 0)),[parameter['batch'],np.shape(p[0])[0],np.shape(p[0])[1],np.shape(p[0])[2]])\n",
        "        q=torch.nn.functional.interpolate(q, size=(parameter['H'],parameter['W']), scale_factor=None, mode='nearest')\n",
        "        graph_tensor = a\n",
        "        edge_index=edges_idx[0].T\n",
        "        edge_weight=edges_wt[0]\n",
        "        batch=parameter['batch']\n",
        "        alpha = parameter['alpha']\n",
        "        m_plus = parameter['m_plus']\n",
        "        m_minus = parameter['m_minus']\n",
        "        lamda = parameter['lamda']\n",
        "        a = a.to(device)\n",
        "        b = b.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "        edge_weight = edge_weight.to(device)\n",
        "        batch = torch.tensor(batch).to(device)\n",
        "        alpha = torch.tensor(alpha).to(device)\n",
        "        m_plus = torch.tensor(m_plus).to(device)\n",
        "        m_minus = torch.tensor(m_minus).to(device)\n",
        "        lamda = torch.tensor(lamda).to(device)\n",
        "        p = [torch.abs(torch.stft(a[i],200,return_complex=True)) for i in range(len(a))]\n",
        "        q = torch.reshape((torch.cat(p,axis = 0)),[batch,np.shape(p[0])[0],np.shape(p[0])[1],np.shape(p[0])[2]])\n",
        "        q=torch.nn.functional.interpolate(q, size=(parameter['H'],parameter['W']), scale_factor=None, mode='nearest')\n",
        "\n",
        "        pred1,pred3,pred2,label_new=model(graph_tensor,edge_index,edge_weight,q,b)\n",
        "        loss = model.loss(pred1,pred2,pred3,q,label_new,b,alpha,m_plus,m_minus,lamda)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss_image += loss\n",
        "        metric=metrics(b,pred1)\n",
        "        acc_epoch += metric[0]\n",
        "        f1_epoch += metric[1]\n",
        "    running_loss_image/=len(train_dl)\n",
        "    acc_epoch /= len(train_dl)\n",
        "    f1_epoch /= len(train_dl)\n",
        "    return model, [acc_epoch,f1_epoch], running_loss_image\n",
        "\n",
        "def validate_one_epoch_net(model, val_dl, device):\n",
        "    running_loss_image=0.0\n",
        "    acc_epoch=0.0\n",
        "    f1_epoch=0.0\n",
        "    dice_epoch=0.0\n",
        "    with torch.no_grad():\n",
        "      for a,b in train_dl:\n",
        "        a=a.float()\n",
        "        a=torch.nn.functional.interpolate(a, size=(parameter['N']), scale_factor=None, mode='nearest')\n",
        "        b=b.float()\n",
        "\n",
        "\n",
        "        ###preparing inputs for model (graph and STFT preparation)\n",
        "        edge_index = [(edge_idx(a[i])) for i in range(len(a))]\n",
        "        edges_idx = torch.reshape((torch.cat(edge_index,axis = 0)),[parameter['batch'],np.shape(edge_index[0])[0],np.shape(edge_index[0])[1]])\n",
        "        edge_weight = [(edge_wt(a[i],edges_idx[i])) for i in range(len(a))]\n",
        "        edges_wt = torch.reshape((torch.cat(edge_weight,axis = 0)),[parameter['batch'],np.shape(edge_weight[0])[0]])\n",
        "        graph_tensor = a\n",
        "        edge_index=edges_idx[0].T\n",
        "        edge_weight=edges_wt[0]\n",
        "        batch=parameter['batch']\n",
        "        alpha = parameter['alpha']\n",
        "        m_plus = parameter['m_plus']\n",
        "        m_minus = parameter['m_minus']\n",
        "        lamda = parameter['lamda']\n",
        "        a = a.to(device)\n",
        "        b = b.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "        edge_weight = edge_weight.to(device)\n",
        "        batch = torch.tensor(batch).to(device)\n",
        "        alpha = torch.tensor(alpha).to(device)\n",
        "        m_plus = torch.tensor(m_plus).to(device)\n",
        "        m_minus = torch.tensor(m_minus).to(device)\n",
        "        lamda = torch.tensor(lamda).to(device)\n",
        "\n",
        "        p = [torch.abs(torch.stft(a[i],200,return_complex=True)) for i in range(len(a))]\n",
        "        q = torch.reshape((torch.cat(p,axis = 0)),[batch,np.shape(p[0])[0],np.shape(p[0])[1],np.shape(p[0])[2]])\n",
        "        q=torch.nn.functional.interpolate(q, size=(parameter['H'],parameter['W']), scale_factor=None, mode='nearest')\n",
        "\n",
        "        pred1,pred3,pred2,label_new=model(graph_tensor,edge_index,edge_weight,q,b)\n",
        "        loss = model.loss(pred1,pred2,pred3,q,label_new,b,alpha,m_plus,m_minus,lamda)\n",
        "        running_loss_image += loss\n",
        "        metric=metrics(b,pred1)\n",
        "        acc_epoch += metric[0]\n",
        "        f1_epoch += metric[1]\n",
        "    running_loss_image/=len(train_dl)\n",
        "    acc_epoch /= len(train_dl)\n",
        "    f1_epoch /= len(train_dl)\n",
        "    return [acc_epoch,f1_epoch], running_loss_image\n",
        "\n",
        "def train_epoches_net(model,train_dl,test_dl,epoches,learn,path,device):\n",
        "    max_accuracy=0.0\n",
        "    for i in range(epoches):\n",
        "        model, metrics_train, loss_train=train_one_epoch_net(model,train_dl,learn,device)\n",
        "        metrics_test, loss_test=validate_one_epoch_net(model,test_dl,device)\n",
        "        print('epoch finished' +\" \" + str(i+1))\n",
        "        print(f'train_loss: {loss_train:.6f}, train_accuracy: {metrics_train[0]:.6f}, train_f1_score: {metrics_train[1]:.6f}')\n",
        "        print(f'test_loss: {loss_test:.6f}, test_accuracy: {metrics_test[0]:.6f}, test_f1_score: {metrics_test[1]:.6f}')\n",
        "        path_final=os.path.join(path,f\"epoch{i}_test_loss_{metrics_test[0]:.4f}.pth\")\n",
        "        torch.save(model.state_dict(), path_final)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training_testing\n",
        "\n",
        "%mkdir eeg\n",
        "train_epoches_net(model,test_dl,test_dl,20,0.001,'/content/eeg',device)"
      ],
      "metadata": {
        "id": "3B4cuS9uUJ2C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}